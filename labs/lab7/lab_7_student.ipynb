{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqMdQGWllsmv"
      },
      "source": [
        "# EE 467 Lab 7: Instruction Set Architecture (ISA) Identification of Program Binaries\n",
        "\n",
        "Welcome to Lab 7 of EE 467! Today we apply the machine learning techniques you’ve learned throughout the course to a practical cybersecurity task: Instruction Set Architecture (ISA) detection. We will use a dataset with **50k Base64-encoded binaries** downloaded from Praetorian’s “Machine Learning Binaries\" challenge web page [1]. Each of the encoded binary string in this dataset consists of 88 characters (66 bytes) on average and belongs to one of the following **twelve architecture types: avr, alphaev56, arm, m68k, mips, mipsel, powerpc, s390, sh4, sparc, x86_64, and xtensa**.\n",
        "\n",
        "We will use two different feature extration models, **byte-histogram+endianness features and byte-level TF-IDF features**, to extract features from the binaries. Using these features, we will **train and test SVM, Logisitic Regression, Decision Tree, and Random Forrest** classification algorithms. As in the previous labs, all algorithms are evaluated by **accuracy, precision, recall and F1-score**.\n",
        "\n",
        "## Preparation\n",
        "\n",
        "Like previous labs, we start by installing all dependencies needed for this lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNqXO4D_lsmx"
      },
      "outputs": [],
      "source": [
        "%pip install numpy scipy scikit-learn termcolor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhrEt8llsmy"
      },
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "We load the raw binary data and their labels from separate files. During loading we decode each Base64 string into bytes, preview a handful of samples in three representations (Base64, hex, and byte-level), deduplicate entries, and compute summary statistics over the full dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xJf binaries-dataset.tar.xz #<---- For unzipping the .zip dataset folder"
      ],
      "metadata": {
        "id": "iXS4um1MrE1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U4MVfdQlsmy"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from collections import Counter\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# Path of binaries and labels files\n",
        "BINARIES_PATH = \"./binaries-dataset/Base64EcncodedBinaries-50k.txt\"\n",
        "LABELS_PATH = \"./binaries-dataset/LabelsOfBinaries-50k.txt\"\n",
        "# Number of samples to display\n",
        "N_DISPLAY_SAMPLES = 5\n",
        "\n",
        "# Binaries and labels\n",
        "binaries = []\n",
        "binaries_set = set()\n",
        "raw_labels = []\n",
        "# Duplicated samples and displayed samples counts\n",
        "dup_count = 0\n",
        "display_count = 0\n",
        "\n",
        "# Decode each Base64-encoded binary into raw bytes, display a few samples in multiple\n",
        "# representations, and deduplicate entries before storing them.\n",
        "# Note: Each byte (8 bits, 256 possible values) is represented by 2 hex characters.\n",
        "with open(BINARIES_PATH) as binaries_file, open(LABELS_PATH) as binaries_label_file:\n",
        "    for line_tmp, label_tmp in zip(binaries_file, binaries_label_file):\n",
        "        # Remove EOL characters\n",
        "        line_eol_removed = line_tmp.rstrip()\n",
        "        raw_label = label_tmp.rstrip()\n",
        "\n",
        "        # [ TODO ]\n",
        "        # 1. Decode Base64-encoded binaries into byte strings\n",
        "        binary_sample = NotImplemented\n",
        "\n",
        "        # Display the first N_DISPLAY_SAMPLES binaries in three representations\n",
        "        if display_count < N_DISPLAY_SAMPLES:\n",
        "            # [ TODO ]\n",
        "            # 2. Encode byte strings as hex strings\n",
        "            hex_encoded = NotImplemented\n",
        "            # 3. Rewrite encoded hex strings in byte-level granularities\n",
        "            #    (i.e. Separate byte data by spaces)\n",
        "            byte_level = NotImplemented\n",
        "\n",
        "            print(colored(\"Base64-encoded string:\", attrs=[\"bold\"]))\n",
        "            print(colored(line_eol_removed, \"red\"))\n",
        "\n",
        "            print(colored(\"Hex-encoded string:\", attrs=[\"bold\"]))\n",
        "            print(colored(hex_encoded, \"blue\"))\n",
        "\n",
        "            print(colored(\"Byte level decomposition:\", attrs=[\"bold\"]))\n",
        "            print(colored(byte_level, \"green\"))\n",
        "            print()\n",
        "\n",
        "            display_count += 1\n",
        "\n",
        "        # Skip duplicates; otherwise add sample to the dataset\n",
        "        if binary_sample not in binaries_set:\n",
        "            binaries.append(binary_sample)\n",
        "            binaries_set.add(binary_sample)\n",
        "            raw_labels.append(raw_label)\n",
        "        else:\n",
        "            dup_count += 1\n",
        "\n",
        "# Count labels\n",
        "labels_info = Counter(raw_labels)\n",
        "min_sample_size = min(labels_info.values())\n",
        "\n",
        "# Compute and print statistics\n",
        "print(colored(\"[[ Dataset Statistics ]]\", attrs=[\"bold\"]))\n",
        "\n",
        "print(\"* Distinct labels:\", labels_info.keys())\n",
        "print(\"* # of samples for all classes:\", labels_info)\n",
        "print(\"* Min # of samples in class:\", min(labels_info.values()))\n",
        "print(\"* # of samples in total:\", sum(labels_info.values()))\n",
        "print(\"* # of duplicates:\", dup_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLZ-vaUWlsmz"
      },
      "source": [
        "The statistics indicate that our dataset is largely balanced — all twelve ISA classes have similar sample counts, so no undersampling or oversampling is needed. We only need to map the string labels to integer indices for use with scikit-learn:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbVQmxqllsmz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Map each unique string label to a consecutive integer (0, 1, 2, ...)\n",
        "raw_labels_to_labels = {label: i for i, label in enumerate(labels_info.keys())}\n",
        "\n",
        "# Build the integer label array used by all scikit-learn models below\n",
        "labels = np.array([raw_labels_to_labels[raw_label] for raw_label in raw_labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiFALuWelsmz"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "In this lab we are going to try three different feature extraction techniques:\n",
        "\n",
        "* Byte-Histogram and Endianness Features\n",
        "* Byte-level 1,2,3-Gram TF-IDF Features\n",
        "* Hex-level (4-bit) 1,2,3-Gram TF-IDF Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynzzqfXzlsmz"
      },
      "source": [
        "### Byte-Histogram and Endianness Features\n",
        "\n",
        "This feature extraction method was originally proposed in [2]. It builds two histograms from raw binary data:\n",
        "\n",
        "1. **Byte-value histogram**: Scans the binary one byte at a time, producing a 256-entry histogram (one bin per possible byte value, 0–255).\n",
        "2. **Endianness histogram**: Scans for four specific two-byte (word) patterns — `00 01`, `01 00`, `ff fe`, and `fe ff` — whose frequency hints at the endianness of the binary data.\n",
        "\n",
        "Both histograms are concatenated and then **normalized** by the total number of bytes in the binary, yielding a 260-dimensional feature vector that is comparable across binaries of different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH2e9bJGlsm0"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Iterator\n",
        "\n",
        "# The four two-byte patterns whose presence signals endianness\n",
        "ENDIANNESS_WORDS = (b\"\\x00\\x01\", b\"\\x01\\x00\", b\"\\xff\\xfe\", b\"\\xfe\\xff\")\n",
        "\n",
        "def pairwise(bstr: bytes) -> Iterator[bytes]:\n",
        "    \"\"\"\n",
        "    Yield every consecutive overlapping byte pair in `bstr`.\n",
        "\n",
        "    Example: b\"uvwxyz\" -> b\"uv\", b\"vw\", b\"wx\", b\"xy\", b\"yz\"\n",
        "    \"\"\"\n",
        "    bstr_iter = iter(bstr)\n",
        "    char_a = next(bstr_iter)\n",
        "\n",
        "    for char_b in bstr_iter:\n",
        "        yield char_a + char_b\n",
        "        char_a = char_b\n",
        "\n",
        "def make_byte_hist_endian_feature(binary: bytes) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return a 260-dim normalized feature vector for a single binary.\n",
        "\n",
        "    The vector is the concatenation of:\n",
        "      - a 256-entry byte-value histogram\n",
        "      - a 4-entry endianness-word histogram\n",
        "    divided element-wise by the binary's byte length.\n",
        "    \"\"\"\n",
        "    # Count occurrences of each byte value (0–255)\n",
        "    byte_hist = np.zeros(256, dtype=int)\n",
        "    for byte_data in binary:\n",
        "        byte_hist[byte_data] += 1\n",
        "\n",
        "    # Count occurrences of the four endianness-indicating word patterns\n",
        "    word_hist = np.zeros(len(ENDIANNESS_WORDS), dtype=int)\n",
        "    for word_data in pairwise(binary):\n",
        "        try:\n",
        "            word_idx = ENDIANNESS_WORDS.index(word_data)\n",
        "            word_hist[word_idx] += 1\n",
        "        except ValueError:\n",
        "            # Not an endianness marker; skip\n",
        "            pass\n",
        "\n",
        "    # Concatenate and normalize by binary length so features are scale-invariant\n",
        "    concat_hist = np.concatenate((byte_hist, word_hist))\n",
        "    return concat_hist / len(binary)\n",
        "\n",
        "feats_byte_hist_endian = np.stack([make_byte_hist_endian_feature(binary) for binary in binaries])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12wMcurNlsm0"
      },
      "source": [
        "### Byte-level 1,2,3-Gram TF-IDF Features\n",
        "\n",
        "This method generalizes the byte-histogram approach using TF-IDF weighting over byte n-grams (unigrams, bigrams, and trigrams). A few implementation notes:\n",
        "\n",
        "* We reuse scikit-learn's `TfidfVectorizer` with the character (`\"char\"`) analyzer so it treats each byte as a single character. Setting the encoding to `latin1` ensures every possible byte value (0–255) maps to a valid character without errors.\n",
        "* For trigrams, we cap the vocabulary with `max_features` to keep feature dimensionality (and training time) manageable.\n",
        "* Unigram+bigram and trigram matrices are merged in the feature dimension using `scipy.sparse.hstack`, matching the approach from homework 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB8dhl5Nlsm0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Limit trigram vocabulary to keep memory and training time manageable\n",
        "TF_IDF_3_MAX_FEATS = 10000\n",
        "\n",
        "# [ TODO ]\n",
        "# Extract Byte-level (1,2,3)-gram TF-IDF features using TfidfVectorizer\n",
        "# 1. Fit character (1,2)-gram TF-IDF model using `binaries` as training data, with `encoding` set to\n",
        "#    \"latin1\". Save transformed features as `feats_tf_idf_12`.\n",
        "feats_tf_idf_12 = NotImplemented\n",
        "\n",
        "# 2. Fit character trigram TF-IDF model using `binaries` as training data, with `encoding` set to\n",
        "#    \"latin1\" and number of features limited by `TF_IDF_3_MAX_FEATS`. Save transformed features as\n",
        "#    `feats_tf_idf_3`.\n",
        "feats_tf_idf_3 = NotImplemented\n",
        "\n",
        "# 3. Concatenate `feats_tf_idf_12` and `feats_tf_idf_3` into `feats_tf_idf_123` using `scipy.sparse.hstack`.\n",
        "feats_tf_idf_123 = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ubLldHglsm0"
      },
      "source": [
        "### Hex-level (4-bit) 1,2,3-Gram TF-IDF Features\n",
        "\n",
        "This method applies the same TF-IDF n-gram approach but operates on the **hex-encoded** representation of each binary. Because each hex character encodes four bits (half a byte), the resulting vocabulary captures finer-grained patterns than the byte-level approach, at the cost of doubling the sequence length. No `latin1` encoding override is needed here since all hex characters are standard ASCII.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2WOcyo3lsm0"
      },
      "outputs": [],
      "source": [
        "# [ TODO ]\n",
        "# Extract Hex-level (1,2,3)-gram TF-IDF features using TfidfVectorizer\n",
        "# 1. Hex-encode all binary data in `binaries` and save them as `binaries_hex`\n",
        "binaries_hex = NotImplemented\n",
        "# 2. Fit character (1,2,3)-gram TF-IDF model using `binaries_hex` as training data. Save transformed features\n",
        "#    as `feats_tf_idf_hex_123`.\n",
        "feats_tf_idf_hex_123 = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSMLfdlplsm1"
      },
      "source": [
        "## Training and Testing Different ML Models\n",
        "\n",
        "Now that we have multiple feature representations, we evaluate each one across four classifiers: **Linear SVM**, **Logistic Regression**, **Decision Tree**, and **Random Forest**. To keep runtimes reasonable with a 50k-sample dataset, we use a stratified 20%/5% train/test split. We report accuracy, precision, recall, and F1-score, as well as the confusion matrix, for both the training and test sets.\n",
        "\n",
        "We start by implementing a shared `train_test_ml_models` function that encapsulates the complete train-evaluate loop so we can reuse it for each feature type:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFsJ3PG4lsm1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Default random seed for reproducibility\n",
        "RNG_SEED = 42\n",
        "\n",
        "@contextmanager\n",
        "def timeit(action: str):\n",
        "    \"\"\"Context manager that prints elapsed wall-clock time for a code block.\"\"\"\n",
        "    start_time = time.time()\n",
        "    print(f\"Timing started for {action} ...\")\n",
        "\n",
        "    yield\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Timing ended for {action}. Elapsed time: {elapsed_time:.2f}s\")\n",
        "\n",
        "def train_test_ml_models(feats: np.ndarray, labels: np.ndarray, train_size: float = 0.2, test_size: float = 0.05,\n",
        "    rng_seed_data: int = RNG_SEED, rng_seed: int = RNG_SEED):\n",
        "    \"\"\"\n",
        "    Train and evaluate four classifiers on the given features and labels.\n",
        "\n",
        "    Uses a stratified train/test split to preserve class balance, then reports\n",
        "    confusion matrices and full classification reports for both subsets.\n",
        "    \"\"\"\n",
        "    # Stratified split keeps class proportions consistent across train and test sets\n",
        "    binaries_train, binaries_test, labels_train, labels_test = train_test_split(\n",
        "        feats, labels, train_size=train_size, test_size=test_size, stratify=labels, random_state=rng_seed_data\n",
        "    )\n",
        "\n",
        "    # List of ML models to train and evaluate\n",
        "    ml_models = {\n",
        "        \"SVM\": LinearSVC(max_iter=200, random_state=rng_seed),\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=rng_seed),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=rng_seed),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=rng_seed)\n",
        "    }\n",
        "\n",
        "    for model_name, model in ml_models.items():\n",
        "        print(colored(f\"[[ Training and Evaluation for {model_name} ]]\", attrs=[\"bold\"]))\n",
        "        print(colored(\"[ Training ]\", attrs=[\"bold\"]))\n",
        "\n",
        "        with timeit(action=f\"training of {model_name}\"):\n",
        "            model.fit(binaries_train, labels_train)\n",
        "\n",
        "        # Compute and show metrics for training and testing sets\n",
        "        for subset_name, binaries_eval, labels_eval in (\n",
        "            (\"Training\", binaries_train, labels_train),\n",
        "            (\"Testing\", binaries_test, labels_test)\n",
        "        ):\n",
        "            preds = model.predict(binaries_eval)\n",
        "\n",
        "            print(colored(f\"[ {subset_name} Set ]\", attrs=[\"bold\"]))\n",
        "\n",
        "            print(\"Confusion matrix:\")\n",
        "            print(confusion_matrix(labels_eval, preds))\n",
        "            print()\n",
        "\n",
        "            print(\"Classification report:\")\n",
        "            print(classification_report(labels_eval, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kdVK-4olsm1"
      },
      "source": [
        "With the training and evaluation routine implemented, let's first run it on the **Byte-Histogram and Endianness Features**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB0Pe4emlsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_byte_hist_endian, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R69HkAMQlsm1"
      },
      "source": [
        "... and for **Byte-level 1,2,3-Gram TF-IDF Features**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXdPs8-Blsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_tf_idf_123, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vio4XsB-lsm1"
      },
      "source": [
        "Let's check the shape of the byte-level TF-IDF feature matrix before deciding on dimensionality reduction:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlvF3gWdlsm1"
      },
      "outputs": [],
      "source": [
        "feats_tf_idf_123.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lj54-uSlsm1"
      },
      "source": [
        "We'll find out that for **Byte-level 1,2,3-Gram TF-IDF Features**, there are more than 32,000 feature dimensions per sample. To ease the computational burden, we apply `KernelPCA` with an RBF kernel to reduce dimensionality to 300 components. Because `KernelPCA` does not natively support sparse input, we first fit it on a randomly-selected 20% subset of the data to reduce memory usage, then transform the full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAgZS6DFlsm1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "KERNEL_PCA_DIMS = 300\n",
        "\n",
        "# KernelPCA is fitted on a 20% stratified subset to reduce memory and runtime.\n",
        "# The full dataset is then transformed using the fitted model.\n",
        "feats_tf_idf_123_kpca_train, _ = train_test_split(\n",
        "    feats_tf_idf_123, train_size=0.2, stratify=labels, random_state=RNG_SEED\n",
        ")\n",
        "\n",
        "with timeit(\"Kernel PCA fitting\"):\n",
        "    kpca_tf_idf_123 = KernelPCA(n_components=KERNEL_PCA_DIMS, kernel=\"rbf\", random_state=RNG_SEED)\n",
        "    kpca_tf_idf_123.fit(feats_tf_idf_123_kpca_train)\n",
        "\n",
        "with timeit(\"Kernel PCA dimensionality reduction\"):\n",
        "    feats_tf_idf_123_kpca = kpca_tf_idf_123.transform(feats_tf_idf_123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnrIYke8lsm1"
      },
      "source": [
        "We can now run training and evaluation using the dimensionality-reduced (300-component) byte-level TF-IDF features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFoT9ISplsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_tf_idf_123_kpca, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKRahyeAlsm1"
      },
      "source": [
        "Finally, let's run training and evaluation on the **Hex-level 1,2,3-Gram TF-IDF Features**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVXvYbLtlsm1"
      },
      "outputs": [],
      "source": [
        "train_test_ml_models(feats_tf_idf_hex_123, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBf9TFdblsm2"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Tech challenge: Machine learning binaries,” Feb 2021. [Online]. Available: https://www.praetorian.com/challenges/machine-learning-challenge/#how-to-play\n",
        "2. J. Clemens, “Automatic classification of object code using machine learning,” Digital Investigation, vol. 14, pp. S156–S162, 2015.\n",
        "3. D. Sahabandu, J. S. Mertoguno and R. Poovendran, \"A Natural Language Processing Approach for Instruction Set Architecture Identification,\" in IEEE Transactions on Information Forensics and Security, vol. 18, pp. 4086-4099, 2023, doi: 10.1109/TIFS.2023.3288456.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}