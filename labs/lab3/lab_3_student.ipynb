{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Xe7qgl6PHU"
      },
      "source": [
        "# EE 467 Lab 3: Dimensionality Reduction and Visualization\n",
        "\n",
        "Starting with lab 3, we will move on to two classes of unsupervised machine learning algorithms-dimensionality reduction and clustering. These algorithms can help us figure out previously unknown patterns in the features without any kind of labels. We will apply these algorithms to the KDD Cup 1999 network intrusion dataset and visualize their results. These results may reflect differences in the characteristics of malicious and normal network connections from an alternative perspective.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will work on the KDD Cup 1999 network intrusion dataset for this lab. The KDD Cup 1999 dataset consists of approximately 4.9 million records of network connections. Here a connection is a sequence of TCP packets starting and ending at some well defined times, during which data flows to and from a source IP address to a target IP address under some well defined protocol. Each connection contains 41 features and is labeled as either normal or an attack, with exactly one specific attack type. The original goal of the contest is to predictive model (i.e. a classifier) capable of distinguishing between malicious connections, called intrusions or attacks, and good normal connections.\n",
        "\n",
        "Attacks in the KDD Cup 1999 dataset fall into four main categories:\n",
        "\n",
        "1. DoS: Denial-of-service attacks, e.g. syn flood.\n",
        "2. R2L: Unauthorized access from a remote machine, e.g. password guessing.\n",
        "3. U2R: Unauthorized access to local superuser (root) privileges, e.g. privilege escalation through buffer overflow attacks.\n",
        "4. Probing: surveillance and other probing, e.g. port scanning.\n",
        "\n",
        "## Pre-processing\n",
        "\n",
        "The KDD cup 1999 network intrusion dataset is huge (~743M) and is way too much to demostrate dimensionality reduction and data visualization. As such, we use a reduced dataset that only contains 10% of the original data. As usual, we load the dataset into memory and preview a few records of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "MLUiXyKb6PHW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Column names for KDD Cup 1999 dataset\n",
        "all_columns = [\n",
        "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\",\n",
        "    \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
        "    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
        "    \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\",\n",
        "    \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
        "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\",\n",
        "    \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
        "    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
        "    \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"\n",
        "]\n",
        "\n",
        "# Load dataset with Pandas\n",
        "kdd_data_10p = pd.read_csv(\"./kddcup.data_10_percent_corrected\", names=all_columns)\n",
        "# Preview a few records\n",
        "kdd_data_10p.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3i_xgGZ6PHX"
      },
      "source": [
        "We can also show statistics (such as mean, median, standard deviation etc.) of each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h11dHNn6PHX"
      },
      "outputs": [],
      "source": [
        "# Show statistics of the dataset\n",
        "kdd_data_10p.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSKomSsL6PHY"
      },
      "source": [
        "But we are probably more interested in the categories and frequencies of different attacks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "oCjzME4u6PHY"
      },
      "outputs": [],
      "source": [
        "# Show the categories and frequencies of different attacks\n",
        "kdd_data_10p[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKthi4Xl6PHY"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "Now that we have loaded the dataset, we can observe two kinds of columns: some columns, such as `src_bytes` and `dst_bytes`, are numerical; some others, like `label`, are categorical, in that there is only a limited number of possible values for them. For dimensionality reduction, we are not considering categorical columns as part of the features for now, as they require special processing. The following code extracts numerical features from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-piOA97H6PHY"
      },
      "outputs": [],
      "source": [
        "# All categorical columns\n",
        "categorical_columns = [\"protocol_type\", \"service\", \"flag\", \"label\"]\n",
        "\n",
        "# Extract numerical columns only and convert DataFrame to float type\n",
        "kdd_features = kdd_data_10p.copy()\n",
        "\n",
        "for column in categorical_columns:\n",
        "    del kdd_features[column]\n",
        "\n",
        "# Preview a few records\n",
        "kdd_features.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxzt01p6PHY"
      },
      "source": [
        "Next, we need to perform **feature scaling** on our features. Notice that for each column, the mean and variance (and thus the distribution) of the data is different from others, causing the unit of each dimension to been different. This poses problems for unsupervised learning algorithms as they treat all dimensions to be equal so that they can compute the distance between samples. Here we will try two ways to solve the problem.\n",
        "\n",
        "The first solution, in which we use `MinMaxScaler`, rescales data of each dimension proportionally to $[0, 1]$ by performing the following transformation:\n",
        "\n",
        "$$\n",
        "x_{minmax} = \\frac{x-\\min(x)}{\\max(x)-\\min(x)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLIYCw0j6PHZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Features obtained from `MinMaxScaler`\n",
        "kdd_feat_minmax = pd.DataFrame()\n",
        "\n",
        "## [ TODO ]\n",
        "# For each column in `kdd_features`\n",
        "for col_name in kdd_features.columns:\n",
        "    # 1) Convert column data to NumPy array and add an extra dimension\n",
        "    # 2) Re-scale data with `MinMaxScaler`\n",
        "    # 3) Update column with scaled result\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Convert features to NumPy array\n",
        "kdd_feat_minmax = kdd_feat_minmax.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIp_8Lrp6PHZ"
      },
      "source": [
        "The second solution, in which we use `StandardScaler` instead, de-mean each dimension by subtracting the average value from original data, and then re-scale data by the standard deviation of the original data:\n",
        "\n",
        "$$\n",
        "x_{std} = \\frac{x - \\overline{x}}{std(x)}\n",
        "$$\n",
        "\n",
        "However, as you will see later, just re-scale data with `StandardScaler` isn't enough as the `StandardScaler` features doesn't look good with dimensionality reduction algorithms. A few **outliers** cause the majority of samples to be visualized on the same side of the graph, which is not helpful for showing the distribution of samples. (Why?)\n",
        "\n",
        "To avoid outliers from affecting the plot of dimensionality reduction, we can detect and remove them in advance. How can we remove connections with at least one dimension with outlier data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jed8QlYY6PHZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features obtained from `StandardScaler`\n",
        "kdd_feat_std = pd.DataFrame()\n",
        "\n",
        "# Remove outliers for `StandardScaler`\n",
        "remove_outliers = True\n",
        "# Outlier threshold (standard deviation as unit)\n",
        "outlier_threshold = 10\n",
        "# Indices of records with outlier values\n",
        "outlier_indices = set()\n",
        "\n",
        "for col_name in kdd_features.columns:\n",
        "    # Obtain column data as 2D NumPy array\n",
        "    col_data = kdd_features[col_name].values.astype(float)[:, None]\n",
        "    # Re-scale data with `StandardScaler`\n",
        "    col_data = StandardScaler().fit_transform(col_data)\n",
        "\n",
        "    if remove_outliers:\n",
        "        ## [ TODO ]\n",
        "        # 1) Get indices of records with outlier column values\n",
        "        # 2) Merge these indices with `outlier_indices`\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Copy rescaled column\n",
        "    kdd_feat_std[col_name] = col_data.squeeze()\n",
        "\n",
        "## [ TODO ]\n",
        "# Remove records with outlier values\n",
        "if remove_outliers:\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Convert features to NumPy array\n",
        "kdd_feat_std = kdd_feat_std.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qny6ArEo6PHZ"
      },
      "source": [
        "We will also attach labels for these samples. For this lab we only care about whether a connection is \"malicious\" (associated with an attack) or not, so we convert the labels column to either 0 (for normal connection) or 1 (for connection associated with an attck):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_4r6C6Q6PHZ"
      },
      "outputs": [],
      "source": [
        "## [ TODO ]\n",
        "# Convert labels to 0 for normal connection and 1 for connection associated with attack\n",
        "y = NotImplemented\n",
        "\n",
        "# Numerical labels for `MinMaxScaler` features\n",
        "y_minmax = y.values\n",
        "\n",
        "# Numerical labels for `StandardScaler` features\n",
        "y_std = y\n",
        "# Removing labels for samples with outlier data for consistency\n",
        "if remove_outliers:\n",
        "    y_std = y.drop(outlier_indices)\n",
        "\n",
        "# Convert labels to NumPy array\n",
        "y_std = y_std.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhdjMHJ96PHZ"
      },
      "source": [
        "To reduce the execution time of dimenionality reduction algorithms, we will only work on a very small portion of samples (5% of the 10% dataset, or in other words, 0.5% of the original data). This is fine as we are mainly demostrating the usage and effects of these algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvTFAtDL6PHZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Further select only 5% of the samples for `MinMaxScaler` features\n",
        "kdd_feat_minmax, _, y_minmax, _ = train_test_split(\n",
        "    kdd_feat_minmax, y_minmax, train_size=0.05, random_state=12345678\n",
        ")\n",
        "\n",
        "# Further select only 5% of the samples for `StandardScaler` features\n",
        "kdd_feat_std, _, y_std, _ = train_test_split(\n",
        "    kdd_feat_std, y_std, train_size=0.05, random_state=12345678\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoMuVTha6PHZ"
      },
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "In data mining and analysis, we often work with sparse, high-dimensional features. There are mainly two problems when directly applying traditional machine learning algorithms to these features: excessive computation power and memory requirements, and failure of learning effectively on such features. Dimensionality reduction algorithms solve these issues by transforming sparse, high-dimensional features into a lower-dimensional space where features are compactly distributed and represented. It also makes features much more intuitive by reducing them to a 2D (or 3D) space, which is much easier to handle for human brains.\n",
        "\n",
        "Before showcasing the effects of dimensionality reduction algorithms, we need a function to visualize transformed features. In the `visualize_samples` function below, we will draw all transformed features on both a 2D and a 3D plot. We will plot normal connection features as green dots and malicious connection features as orange dots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "htVHJ6mp6PHa"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def visualize_samples(samples, labels, title=\"Samples visualization\", colors=[\"green\", \"orange\"]):\n",
        "    \"\"\" Visualize first three dimensions of samples. \"\"\"\n",
        "    # Convert colors to NumPy array\n",
        "    colors = np.array(colors)\n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "    # Plot first two dimensions on a 2D plot\n",
        "    ax_2d = fig.add_subplot(1, 2, 1)\n",
        "    ax_2d.set_title(f\"{title} [2D]\")\n",
        "    ax_2d.scatter(samples[:, 0], samples[:, 1], c=colors[labels])\n",
        "\n",
        "    # Plot first three dimensions on a 3D plot\n",
        "    ax_3d = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
        "    ax_3d.set_title(f\"{title} [3D]\")\n",
        "    ax_3d.scatter(samples[:, 0], samples[:, 1], samples[:, 2], c=colors[labels])\n",
        "\n",
        "    print(fig)\n",
        "\n",
        "# Visualize `StandardScaler` features (without dimensionality reduction)\n",
        "visualize_samples(kdd_feat_std, y_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2ewqhsM6PHa"
      },
      "source": [
        "We will also use the following `timeit` function to monitor the training time of each algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PyC6wRG6PHa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def timeit(action=\"Timing\"):\n",
        "    # Record start time\n",
        "    print(f\"{action} started...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Execute task\n",
        "    yield\n",
        "\n",
        "    # Compute and show elapsed time\n",
        "    elapsed_time = time.time()-start_time\n",
        "    print(f\"{action} completed. Elapsed time: {elapsed_time:.2f}s\")\n",
        "\n",
        "# Test timing function by sleeping for 1 second\n",
        "with timeit(\"Testing timing function\"):\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvAuiwGA6PHa"
      },
      "source": [
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is a simple yet powerful technique used for dimensionality reduction. It is based on Singular Value Decomposition (SVD) of a matrix. Suppose matrix $\\mathbf{M}$ of shape $m \\times n$ represents $n$-dimensional features of $m$ samples, then according to SVD it can be decomposed into:\n",
        "\n",
        "$$\n",
        "\\mathbf{M} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^*\n",
        "$$\n",
        "\n",
        "<div style=\"margin-top: 10px;\">\n",
        "    <img src=\"attachment:svd.png\" width=\"25%\" />\n",
        "</div>\n",
        "\n",
        "Where $\\mathbf{U}$ is of shape $m \\times m$ and its columns are called left singular vectors. Similarly, $\\mathbf{V}^*$ is of shape $n \\times n$ and its rows are called right singular vectors. $\\mathbf{\\Sigma}$ is a diagonal matrix of shape $m \\times n$ and its diagonal elements are called singular values. For convenience reasons, here we call the $i$-th left and right singular vectors as well as corresponding singular value the $i$-th component.\n",
        "\n",
        "To reduce the dimension of matrix $\\mathbf{M}$, we first reorder $\\mathbf{U}$, $\\mathbf{\\Sigma}$ and $\\mathbf{V}^*$ by the singular value of each component, which can be seen as the \"importance\" of each component for the original data. Then, we truncate $\\mathbf{U}$, $\\mathbf{\\Sigma}$ and $\\mathbf{V}^*$ such that only the first $k (k < n)$ components are preserved. This gives as $\\mathbf{U'}$ of shape $m \\times k$, $\\mathbf{\\Sigma'}$ of shape $k \\times k$ and $\\mathbf{V'}^*$ of shape $k \\times n$. Then, the result of PCA is given by:\n",
        "\n",
        "$$\n",
        "\\mathbf{M'} = \\mathbf{U'} \\sqrt{\\mathbf{\\Sigma}}\n",
        "$$\n",
        "\n",
        "Below is the visualization of PCA result on two-dimensional data. The arrow shows the two components of PCA and the relative length of the arrow indicates the \"importance\" of each component:\n",
        "\n",
        "<div style=\"margin-top: 10px;\">\n",
        "    <img src=\"attachment:pca-example.png\" width=\"50%\" />\n",
        "</div>\n",
        "\n",
        "Now, we will perform regular PCA on the `StandardScaler` features we have generated above. Note here we use `IncrementalPCA` rather than `PCA` for faster training, but the result should be roughly the same:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkQtaKbF6PHa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "## [ TODO ]\n",
        "# Perform regular PCA on `kdd_feat_std`\n",
        "raise NotImplementedError\n",
        "\n",
        "# Plot regular PCA result\n",
        "visualize_samples(kdd_feat_pca, y_std, title=\"Regular PCA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzeOeG2G6PHa"
      },
      "source": [
        "### Kernel PCA\n",
        "\n",
        "Before talking about kernel PCA, we need to introduce kernel method. Here is a simple introduction of kernel method:\n",
        "\n",
        "> ... With kernel functions, we implictly map sample features (represented as points) into a higher-dimensional space, where the inner product of transformed sample features can be efficiently computed using the kernel function. Kernel method can be used in conjunction with linear regression algorithm or its varients to deal with non-linear mapping. It can also be used with SVM to make sample features linear serapable in an alternative space.\n",
        "\n",
        "Like kernel SVM, in kernel PCA we implicitly map sample features into a higher-dimensional alternative space. This might sound contradictive to the purpose of dimensionality reduction, but what we want to achieve here is similar to the SVM case: to make transformed features linear separable. Then, we will perform PCA on transformed features to bring the number of dimensions down. Compared with regular PCA, this might separate features from different classes better and make visualization more intuitive.\n",
        "\n",
        "<div>\n",
        "    <img src=\"attachment:kernel-svm.png\" width=\"50%\" />\n",
        "</div>\n",
        "\n",
        "For kernel PCA (and all other kernel-based algorithms), the kernel functions are usually non-linear functions and must satisfy [Mercer's Theorm](https://en.wikipedia.org/wiki/Mercer%27s_theorem), which requires them to be symmetric positive-semidefinite functions. Here we introduce a few common kernel functions:\n",
        "\n",
        "* Polynomial kernel: suppose we have two sample vector $\\mathbf{x}$ and $\\mathbf{y}$, then the polynomial kernel function $K(\\mathbf{x}, \\mathbf{y})$ is defined as:\n",
        "\n",
        "$$\n",
        "K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^T \\mathbf{y} + c)^d\n",
        "$$\n",
        "\n",
        "Where $c \\ge 0$ is a constant controlling the influence of each dimension and $d$ determines number of dimension of the transformed space.\n",
        "\n",
        "* Radial basis function (RBF) kernel: the radial basis function kernel is defined as:\n",
        "\n",
        "$$\n",
        "K(\\mathbf{x}, \\mathbf{y}) = \\exp(\\frac{-||\\mathbf{x} - \\mathbf{y}||_2^2}{2 \\sigma^2})\n",
        "$$\n",
        "\n",
        "Where $\\sigma$ is a hyper-parameter. RBF kernel corresponds to an infinite-dimensional transformed space.\n",
        "\n",
        "* Sigmoid kernel: the sigmoid kernel is defined as:\n",
        "\n",
        "$$\n",
        "K(\\mathbf{x}, \\mathbf{y}) = \\tanh(\\mathbf{x}^T \\mathbf{y} + c)\n",
        "$$\n",
        "\n",
        "Where $c \\ge 0$ is a constant controlling the influence of each dimension. Like RBF kernel, sigmoid kernel also corresponds to infinite-dimensional transformed space.\n",
        "\n",
        "This time, let's try kernel PCA on the `MinMaxScaler` features instead. We will use different kernel functions and also try a few hyper-parameter settings for each kernel function type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XObc1mYM6PHa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Polynomial Kernel PCA (d = 3)\n",
        "pca_poly_3 = KernelPCA(n_components=3, kernel=\"poly\")\n",
        "with timeit(\"Kernel PCA (Polynomial; d = 3)\"):\n",
        "    kdd_feat_pca_poly_3 = pca_poly_3.fit_transform(kdd_feat_minmax)\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Try a larger `d` for polynomial kernel (e.g. `d > 5`). Is your visualization result different?\n",
        "# 2) Try other kernel functions, such as RBF or sigmoid kernel. Is your visualization result different?\n",
        "raise NotImplementedError\n",
        "\n",
        "# Plot kernel PCA result\n",
        "visualize_samples(kdd_feat_pca_poly_3, y_minmax, title=\"Kernel PCA (Polynomial; d = 3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4-NaAdC6PHa"
      },
      "source": [
        "### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction algorithm that works well on visualizing high-dimensional, non-linear features. It is widely applied in image processing, natural language processing (NLP), genomic data and speech recognition. Roughly speaking, t-SNE models the distribution of neighbors for each feature in the high-dimensional space as a Gaussian distribution. It then tries to find a t-distribution in the target, low-dimensional space that resembles the high-dimensional one as much as possible. Finally, t-SNE random places target features in the low-dimensional space, such that their pairwise distances conform to the t-distribution solved above. You can take a look at the references if you are interested in the details of the algorithm.\n",
        "\n",
        "For t-SNE, we will run it on the `StandardScaler` features and see if it can visualize features better than PCA. We will also try different perplexity values, which control how many nearest neighbors are used to model the neighbor Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OyFq8wHL6PHa"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# t-SNE (Perplexity set to 80)\n",
        "t_sne_1 = TSNE(n_components=3, perplexity=80, init=\"pca\", random_state=12345)\n",
        "with timeit(\"t-SNE (Perplexity = 80)\"):\n",
        "    kdd_feat_t_sne_1 = t_sne_1.fit_transform(kdd_feat_std)\n",
        "\n",
        "# t-SNE (Perplexity set to 40)\n",
        "t_sne_2 = TSNE(n_components=3, init=\"pca\", random_state=12345)\n",
        "with timeit(\"t-SNE (Perplexity = 30)\"):\n",
        "    kdd_feat_t_sne_2 = t_sne_2.fit_transform(kdd_feat_std)\n",
        "\n",
        "# Plot t-SNE result\n",
        "visualize_samples(kdd_feat_t_sne_1, y_std, title=\"t-SNE (Perplexity = 80)\")\n",
        "visualize_samples(kdd_feat_t_sne_2, y_std, title=\"t-SNE (Perplexity = 30)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1niTt2f6PHa"
      },
      "source": [
        "## References\n",
        "\n",
        "1. KDD Cup 1999 Dataset: https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
        "2. Principal Component Analysis (PCA): https://en.wikipedia.org/wiki/Principal_component_analysis\n",
        "3. Singular Value Decomposition (SVD): https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
        "4. Tutorial: Principal Components Analysis (PCA): https://lazyprogrammer.me/tutorial-principal-components-analysis-pca/\n",
        "5. Kernel method: https://en.wikipedia.org/wiki/Kernel_method\n",
        "6. Mercer's theorm: https://en.wikipedia.org/wiki/Mercer%27s_theorem\n",
        "7. Kernel PCA: https://en.wikipedia.org/wiki/Kernel_principal_component_analysis\n",
        "8. t-distributed Stochastic Neighbor Embedding (t-SNE): https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
        "9. StatQuest: t-SNE, Clearly Explained: https://www.youtube.com/watch?v=NEaUSP4YerM\n",
        "10. Introduction to t-SNE: https://www.datacamp.com/community/tutorials/introduction-t-sne"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}