{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCa0_XoXj40Q"
      },
      "source": [
        "# EE 467 Homework 1: Twitter Spam Detection\n",
        "\n",
        "Welcome to the first homework of EE 467! Here we will apply the ML pipline introduced in the second lab to **Twitter spam message detection problems**. In the past few years, social medias like Twitter has become the target of spam posting. Automatic spam posts can not only affect user experience, but they can also spread misinformation and subtly affect public opinions.\n",
        "\n",
        "In this homework, we will work on a mini Twitter message dataset extracted from the CRESCI-2017 dataset. This dataset contains 50000 Twitter messages, half of which are sent by real Twitter users and half of which generated by spam bots. Your task is to train classifiers that can tell these two kinds of messages apart. This time, you'll go deeper and try multiple feature extractors and classifiers. Like the labs, you will need to fill out blanks in code cells. You are also going to answer a few questions and **optionally** work on improvements to existing pipeline completely on your own.\n",
        "\n",
        "## Before You Start\n",
        "\n",
        "1. You can discuss on Slack if you have questions and want to seek help; however, please try your best to **limit the scope of your question** and **avoid asking directly for answers**. You should also **avoid copy-pasting answers and code** from others.\n",
        "2. You **may use AI assistants** (e.g., ChatGPT, Gemini, etc.) to support your work. If you do, you must **clearly acknowledge** this by stating:\n",
        "\n",
        "- **The name of the AI assistant**, and  \n",
        "- **How it was used** (e.g., brainstorming, clarifying concepts, debugging, editing, generating examples)\n",
        "\n",
        "You **must not copy and paste** AI-generated content directly into your submission without **fully understanding** what it does and being able to **explain it in your own words**.\n",
        "\n",
        "3. You can **optionally work on this homework with one other student of this course as a team**, but **each of you needs to submit the homework individually**. If you choose to form a group, please include the name of your teammate here: _(Name of your teammate)_.\n",
        "\n",
        "## Feature Extraction\n",
        "\n",
        "We will begin our ML pipeline with feature extraction in this homework, as the dataset is good enough and does not need any special pre-processing. As usual, we will load the dataset into memory, extract samples features into vectors and split the dataset into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mpUDnGbSj40R"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "text",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "label",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "f8f962be-65d8-4d8b-9e3b-9055a8196a43",
              "rows": [
                [
                  "0",
                  "@baydiangirl it definitely moved some church folks cause I have a audience &amp; readers who engage &amp; I can speak the gospel w/o being a pastor.",
                  "0"
                ],
                [
                  "1",
                  "@suebob Looks like #TXlege made up new rules for the space/time continuum. #midnightmeansmidnight",
                  "0"
                ],
                [
                  "2",
                  "RT @saleemchat: #WJChat #A1B 5 yrs: People skills will be in demand as our primary interactions will be mediated by technology. #journalism",
                  "0"
                ],
                [
                  "3",
                  "@SajidaBalouch Huhhhhh ",
                  "0"
                ],
                [
                  "4",
                  "@Dukester_94 im the lad of all bibles",
                  "0"
                ],
                [
                  "5",
                  "@agraceh13 am I included in this lol",
                  "0"
                ],
                [
                  "6",
                  "harrys hair is so pretty ",
                  "0"
                ],
                [
                  "7",
                  "RT @Alex_GI7: Vamos PSg",
                  "0"
                ],
                [
                  "8",
                  "RT @lemondefr: Les Â«Buffalo SoldiersÂ», ces oubliÃ©s noirs des conquÃªtes amÃ©ricaines http://t.co/4oItgK5SIw RÃ©cit sur @LeMonde_Afrique http:/â¦",
                  "0"
                ],
                [
                  "9",
                  "sore throat, headache, feeling sick, today seriously cant get any better.. #sick #fml",
                  "0"
                ],
                [
                  "10",
                  "RT @KingJohnLove: No heterosexual man is gonna say \"ew she's darkskin\" or \"ew she's white\" if a woman's beautiful she's beautiful.",
                  "0"
                ],
                [
                  "11",
                  "If you don't eat, you get ate....Better get the fuck out my way..before you end up on plate",
                  "0"
                ],
                [
                  "12",
                  "RT @Apriim: look at the shit we're calling love these days.",
                  "0"
                ],
                [
                  "13",
                  "@MimiAKA08 I know! Dried up like a raisin in the sun. Smh.",
                  "0"
                ],
                [
                  "14",
                  "RT @davidisthegoon: been in the \"talking\" stage 69 thousand times in my life ",
                  "0"
                ],
                [
                  "15",
                  "At this point I should wear what I please",
                  "0"
                ],
                [
                  "16",
                  "Death by heart break.",
                  "0"
                ],
                [
                  "17",
                  "RT @lucyliz: Kaelyn is trying to potty train Sir Isaac. \"Pee pee goes in here. Right here. Are you even looking? LOOK AT ME!\"",
                  "0"
                ],
                [
                  "18",
                  "I'm so proud of you @ShawnMendes #ShawnOnCONAN #StitchesTVTakeover â¤ï¸x13",
                  "0"
                ],
                [
                  "19",
                  "@Cristian_veloz Bitch they usually $20. I'm being nice and giving it for $10 ",
                  "0"
                ],
                [
                  "20",
                  "RT @manduhhhpleasee: Springfield friends at ship this weekend #hype @mikebarbieri2 @Matty_Light23",
                  "0"
                ],
                [
                  "21",
                  "RT @gminayolopez: Lo que iguala y diferencia a los #millennials y a la GeneraciÃ³n X delante del #mÃ³vil http://t.co/LqNTI9ATPH",
                  "0"
                ],
                [
                  "22",
                  "â@1997daysu: ",
                  "0"
                ],
                [
                  "23",
                  "@ForrestIsLove nah that's not our definition ",
                  "0"
                ],
                [
                  "24",
                  "@MarleyThirteen thoose retweets where so stupid I almost died",
                  "0"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 25
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@baydiangirl it definitely moved some church f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@suebob Looks like #TXlege made up new rules f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @saleemchat: #WJChat #A1B 5 yrs: People ski...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@SajidaBalouch Huhhhhh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Dukester_94 im the lad of all bibles</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@agraceh13 am I included in this lol</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>harrys hair is so pretty</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RT @Alex_GI7: Vamos PSg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RT @lemondefr: Les Â«Buffalo SoldiersÂ», ces o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sore throat, headache, feeling sick, today ser...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RT @KingJohnLove: No heterosexual man is gonna...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>If you don't eat, you get ate....Better get th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RT @Apriim: look at the shit we're calling lov...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>@MimiAKA08 I know! Dried up like a raisin in t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>RT @davidisthegoon: been in the \"talking\" stag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>At this point I should wear what I please</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Death by heart break.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>RT @lucyliz: Kaelyn is trying to potty train S...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I'm so proud of you @ShawnMendes #ShawnOnCONAN...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>@Cristian_veloz Bitch they usually $20. I'm be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>RT @manduhhhpleasee: Springfield friends at sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>RT @gminayolopez: Lo que iguala y diferencia a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>â@1997daysu:</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>@ForrestIsLove nah that's not our definition</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>@MarleyThirteen thoose retweets where so stupi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  label\n",
              "0   @baydiangirl it definitely moved some church f...      0\n",
              "1   @suebob Looks like #TXlege made up new rules f...      0\n",
              "2   RT @saleemchat: #WJChat #A1B 5 yrs: People ski...      0\n",
              "3                             @SajidaBalouch Huhhhhh       0\n",
              "4               @Dukester_94 im the lad of all bibles      0\n",
              "5                @agraceh13 am I included in this lol      0\n",
              "6                           harrys hair is so pretty       0\n",
              "7                             RT @Alex_GI7: Vamos PSg      0\n",
              "8   RT @lemondefr: Les Â«Buffalo SoldiersÂ», ces o...      0\n",
              "9   sore throat, headache, feeling sick, today ser...      0\n",
              "10  RT @KingJohnLove: No heterosexual man is gonna...      0\n",
              "11  If you don't eat, you get ate....Better get th...      0\n",
              "12  RT @Apriim: look at the shit we're calling lov...      0\n",
              "13  @MimiAKA08 I know! Dried up like a raisin in t...      0\n",
              "14  RT @davidisthegoon: been in the \"talking\" stag...      0\n",
              "15          At this point I should wear what I please      0\n",
              "16                              Death by heart break.      0\n",
              "17  RT @lucyliz: Kaelyn is trying to potty train S...      0\n",
              "18  I'm so proud of you @ShawnMendes #ShawnOnCONAN...      0\n",
              "19  @Cristian_veloz Bitch they usually $20. I'm be...      0\n",
              "20  RT @manduhhhpleasee: Springfield friends at sh...      0\n",
              "21  RT @gminayolopez: Lo que iguala y diferencia a...      0\n",
              "22                                    â@1997daysu:       0\n",
              "23      @ForrestIsLove nah that's not our definition       0\n",
              "24  @MarleyThirteen thoose retweets where so stupi...      0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!tar -xf twitter-mini.tar.xz\n",
        "\n",
        "## [ \n",
        "# 1) Load all data from `twitter-mini.csv` into variable `data`\n",
        "data = pd.read_csv('twitter-mini.csv')\n",
        "# 2) Preview first few samples\n",
        "data.head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t21bJaUzj40S"
      },
      "source": [
        "In lab 2 we have learned how to use the `CountVectorizer` feature extractor, which counts the occurance of each token (letter here) in a sample (Twitter message text here). This time, we will also try `TfidfVectorizer` and see which feature extractor performs better. Here we will introduce the algorithm it implemented, **TF-IDF**:\n",
        "\n",
        "> **TF-IDF** is the abbreviation for **term frequency–inverse document frequency**, and is defined as the product of the two frequency. The most classical version of TF-IDF is:\n",
        ">\n",
        "> $$\n",
        "tf-idf(t) = tf(t, d) \\cdot idf(t, D) = \\frac{f_{t,d}}{|d|} \\log \\frac{|D|}{|D_t|}\n",
        "$$\n",
        ">\n",
        "> Where $t$ is the term, $D$ is the collection of all documents, $d$ is one of the document that contains $t$ and $D_t$ is the subset of documents that contains term $t$. The advantage of TF-IDF is that frequent and useless words, such as \"the\", \"a\" and \"an\" will receive lower weight, since they appear in almost all documents. On the other hand, unique yet frequent (in any $d \\in D_t$) words will get higher weight. Research has shown that TF-IDF is easy to implement and performs relatively well.\n",
        "\n",
        "Before we run these feature extractors on our dataset, we need to convert each message into a sequence of terms. Since Twitter messages often contain emojis, abbreviations, hashtags and other composition of characters, we can't use the word-level analyzer as we have done in lab 2. Instead, we will generate **character N-grams** from these messages.\n",
        "\n",
        "> A **character N-gram** consists of N consecutive characters. Here we show all character 1, 2, 3 and 4-grams for word \"cold\":\n",
        "> <img src=\"attachment:char-ngram.jpg\" width=\"50%\" />\n",
        "\n",
        "Both `CountVectorizer` and `TfidfVectorizer` provide support for character N-grams. Refer to `sklearn`'s documents to build a character N-gram analyzer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dxByYcJ6j40S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features with `CountVectorizer`...\n",
            "Extracting features with `TfidfVectorizer`...\n",
            "Completed.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# 1) Extract features of all messages into `feat1` with `CountVectorizer` using CHARACTER trigrams\n",
        "# 2) Repeat with `TfidfVectorizer` and save generated features as `feat2`\n",
        "print(\"Extracting features with `CountVectorizer`...\")\n",
        "cv = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "feat1 = cv.fit_transform(data['text'])\n",
        "\n",
        "print(\"Extracting features with `TfidfVectorizer`...\")\n",
        "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "feat2 = tfidf.fit_transform(data['text'])\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7FRYR00j40T"
      },
      "source": [
        "We will also try combining two kinds of feature generated above into a single feature. We will use `scipy.sparse.hstack` to concatenate corresonding features for each sample. Remember, concatenated features matrix should have same amount of samples as `feat1` and `feat2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5QjerxF7j40T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token count feature matrix shape: (50000, 79578)\n",
            "TF-IDF feature matrix shape: (50000, 79578)\n",
            "Combined feature matrix shape: (50000, 159156)\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "\n",
        "print(\"Token count feature matrix shape:\", feat1.shape)\n",
        "print(\"TF-IDF feature matrix shape:\", feat2.shape)\n",
        "\n",
        "# Concatenate token count features and TF-IDF features on the feature dimension,\n",
        "# name the combined feature matrix `feat_comb`.\n",
        "feat_comb = scipy.sparse.hstack([feat1, feat2])\n",
        "print(\"Combined feature matrix shape:\", feat_comb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5hMXCezj40T"
      },
      "source": [
        "Finally, let's take a look at the labels. In our dataset, 0 represents messages from real Twitter users and 1 represents spam messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trteOuMnj40T"
      },
      "outputs": [],
      "source": [
        "y = data[\"label\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU8JwW02j40T"
      },
      "source": [
        "## Training\n",
        "\n",
        "As usual, we will start by doing a 80% / 20% training-test split. Like lab 2, we suggest you to **use the same random seed for all splits** for comparable and reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NdvLp3Z1j40U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spliting token count features...\n",
            "Spliting TF-IDF features...\n",
            "Spliting combined features...\n",
            "Completed.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1) Split token count features into 80/20 training and test sets\n",
        "# 2) Split TF-IDF features into 80/20 training and test sets\n",
        "# 3) Split combined features into 80/20 training and test sets\n",
        "\n",
        "# Token count features\n",
        "print(\"Spliting token count features...\")\n",
        "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(\n",
        "    feat1, y, test_size=0.2, random_state=67\n",
        ")\n",
        "# TF-IDF features\n",
        "print(\"Spliting TF-IDF features...\")\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
        "    feat2, y, test_size=0.2, random_state=67\n",
        ")\n",
        "\n",
        "# Combined features\n",
        "print(\"Spliting combined features...\")\n",
        "X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(\n",
        "    feat_comb, y, test_size=0.2, random_state=67\n",
        ")\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgGEBXFtj40U"
      },
      "source": [
        "We are now ready to train **binary classifiers** for spam message detection. Apart from the `LogisticRegression` classifier you have already seen and used during Lab 1, we will try a few more classifiers:\n",
        "\n",
        "* `MultinomialNB` (Naive Bayes):  \n",
        "  This classifier is based on the **Naive Bayes** assumption that, *given the class label*, individual features are conditionally independent.  \n",
        "  For text classification, `MultinomialNB` is used with **token count** or **TF-IDF-like** features.\n",
        "\n",
        "  It models the probability of a message belonging to a class $ y \\in \\{\\text{spam}, \\text{ham}\\} $ as:\n",
        "\n",
        "  $$\n",
        "  P(y \\mid \\mathbf{x}) \\propto P(y)\\prod_{j=1}^{d} P(x_j \\mid y)\n",
        "   $$\n",
        "\n",
        "  where $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$ is the feature vector (e.g., word counts), and $P(x_j \\mid y)$ is learned from training data.  \n",
        "  The predicted label is chosen using:\n",
        "\n",
        "   $$\n",
        "  y_{pred} = \\arg\\max_y \\; P(y \\mid \\mathbf{x})\n",
        "   $$\n",
        "\n",
        "  In practice, the computation is done in the **log domain** to avoid numerical underflow:\n",
        "\n",
        "  $$\n",
        "  y_{pred} = \\arg\\max_y \\left( \\log P(y) + \\sum_{j=1}^{d} x_j \\log P(x_j \\mid y) \\right)\n",
        "  $$\n",
        "\n",
        "* `LinearSVC`:  \n",
        "  This classifier is based on **linear SVM**, a popular algorithm for binary classification. The linear SVM algorithm treats the feature vector of each sample as a point in a linear space, and tries to find a hyperplane that separates points of the two classes as well as possible.  \n",
        "\n",
        "  In practice, we optimize a **squared hinge loss with regularization**, since the features may not be perfectly linearly separable:\n",
        "\n",
        "   $$\n",
        "  l_{hinge} = \\max(0, y(1 - \\mathbf{x}^T \\mathbf{w}))^2 + \\beta ||\\mathbf{w}||_2^2\n",
        "   $$\n",
        "\n",
        "  Similarly, the classification result is determined by:\n",
        "\n",
        "  $$\n",
        "  y_{pred} = sign(1 - \\mathbf{x}^T \\mathbf{w})\n",
        " $$\n",
        "\n",
        "We have three kinds of features (**token count**, **TF-IDF**, and **combined**) and three kinds of classifiers. Here we will only try three combinations and compare their performance:\n",
        "\n",
        "* `MultinomialNB` on token count features  \n",
        "* `LogisticRegression` on TF-IDF features  \n",
        "  - Try increasing the number of iterations if a warning of non-convergence pops up  \n",
        "* `LinearSVC` on combined features  \n",
        "  - For linear SVM, you can ignore the non-convergence warning as it does not affect performance  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TdeGmMHYj40U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Naive Bayes classifier on token count training set...\n",
            "Training logistic regression classifier on TF-IDF training set...\n",
            "Training SVM classifier on combined features training set...\n",
            "Completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# 1) Train a `MultinomialNB` classifier called `model1` on token count training set\n",
        "# 2) Train a `LogisticRegression` classifier called `model2` on TF-IDF training set\n",
        "# 3) Train a `LinearSVC` classifier called `model_comb` on combined features training set\n",
        "\n",
        "print(\"Training Naive Bayes classifier on token count training set...\")\n",
        "model1 = MultinomialNB()\n",
        "model1.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "print(\"Training logistic regression classifier on TF-IDF training set...\")\n",
        "model2 = LogisticRegression(max_iter=1000)\n",
        "model2.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "print(\"Training SVM classifier on combined features training set...\")\n",
        "model_comb = LinearSVC()\n",
        "model_comb.fit(X_train_comb, y_train_comb)\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZYU83qj40U"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Finally, it's time to evaluate our models. You probably noticed that we have repeated similar code for different feature extraction and classification models. To avoid that, we will use `for` loops to make evaluation simpler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ovl_tiKmj40U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================\n",
            "1. Naive Bayes on Token count:\n",
            "\n",
            "Training metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95     20055\n",
            "           1       0.99      0.91      0.95     19945\n",
            "\n",
            "    accuracy                           0.95     40000\n",
            "   macro avg       0.95      0.95      0.95     40000\n",
            "weighted avg       0.95      0.95      0.95     40000\n",
            "\n",
            "Training confusion matrix:\n",
            " [[19783   272]\n",
            " [ 1752 18193]] \n",
            "\n",
            "Train accuracy: 0.9494 \n",
            "\n",
            "Test metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95      4945\n",
            "           1       0.99      0.91      0.95      5055\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[4875   70]\n",
            " [ 458 4597]] \n",
            "\n",
            "Test Accuracy: 0.9472\n",
            "====================\n",
            "\n",
            "====================\n",
            "2. Logistic on TF-IDF:\n",
            "\n",
            "Training metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97     20055\n",
            "           1       0.99      0.94      0.97     19945\n",
            "\n",
            "    accuracy                           0.97     40000\n",
            "   macro avg       0.97      0.97      0.97     40000\n",
            "weighted avg       0.97      0.97      0.97     40000\n",
            "\n",
            "Training confusion matrix:\n",
            " [[19908   147]\n",
            " [ 1211 18734]] \n",
            "\n",
            "Train accuracy: 0.96605 \n",
            "\n",
            "Test metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.95      4945\n",
            "           1       0.99      0.92      0.95      5055\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.96      0.95      0.95     10000\n",
            "weighted avg       0.96      0.95      0.95     10000\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[4876   69]\n",
            " [ 392 4663]] \n",
            "\n",
            "Test Accuracy: 0.9539\n",
            "====================\n",
            "\n",
            "====================\n",
            "3. SVM on Combined:\n",
            "\n",
            "Training metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     20055\n",
            "           1       1.00      1.00      1.00     19945\n",
            "\n",
            "    accuracy                           1.00     40000\n",
            "   macro avg       1.00      1.00      1.00     40000\n",
            "weighted avg       1.00      1.00      1.00     40000\n",
            "\n",
            "Training confusion matrix:\n",
            " [[20054     1]\n",
            " [   25 19920]] \n",
            "\n",
            "Train accuracy: 0.99935 \n",
            "\n",
            "Test metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.95      4945\n",
            "           1       0.95      0.95      0.95      5055\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[4697  248]\n",
            " [ 277 4778]] \n",
            "\n",
            "Test Accuracy: 0.9475\n",
            "====================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "features = [(\"Token count\", feat1), (\"TF-IDF\", feat2), (\"Combined\", feat_comb)]\n",
        "models = [(\"Naive Bayes\", model1), (\"Logistic\", model2), (\"SVM\", model_comb)]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "for i, ((feat_name, feat), (model_name, model)) in enumerate(zip(features, models)):\n",
        "    # Begin of metrics for combination\n",
        "    print(\"====================\")\n",
        "    print(f\"{i+1}. {model_name} on {feat_name}:\\n\")\n",
        "\n",
        "    # 1) Split features into training and test set\n",
        "    # (Note: you should use the same random number seed as the one in the training-test split above)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        feat, y, test_size=0.2, random_state=67\n",
        "    )\n",
        "\n",
        "    # 2) Predict labels for training data\n",
        "    pred_train = model.predict(X_train)\n",
        "\n",
        "    # 3) Print metrics for training set (precision, recall and F1 metrics)\n",
        "    print(\"Training metrics:\", classification_report(y_train, pred_train))\n",
        "    # Confusion matrix\n",
        "    print(\"Training confusion matrix:\\n\", confusion_matrix(y_train, pred_train), \"\\n\")\n",
        "    # Accuracy\n",
        "    print(\"Train accuracy:\", accuracy_score(y_train, pred_train), \"\\n\")\n",
        "\n",
        "    # 4) Predict labels for test data\n",
        "    pred_test = model.predict(X_test)\n",
        "\n",
        "    # 5) Print metrics for test set (precision, recall and F1 metrics)\n",
        "    print(\"Test metrics:\", classification_report(y_test, pred_test))\n",
        "    # Confusion matrix\n",
        "    print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, pred_test), \"\\n\")\n",
        "    # Accuracy\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, pred_test))\n",
        "\n",
        "    # End of metrics for combination\n",
        "    print(\"====================\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydHWRHwj40V"
      },
      "source": [
        "# Questions\n",
        "Please answer the following questions. You can just write down the answer below each question in this cell.\n",
        "\n",
        "\n",
        "1. Please compute `tf-idf` of word-level **bigrams** for the following three messages **by hand**. You should use the TF-IDF definition introduced in class.\n",
        "\n",
        "  * Message 1: \"vegetables are good for health\"\n",
        "  * Message 2: \"fruits are good source of vitamins\"\n",
        "  * Message 3: \"proteins are good for health\"\n",
        "\n",
        "  Compute and sort intermediate and final results for **all occurring bigrams in the following order**:\n",
        "  \n",
        "  ```\n",
        "  [\"vegetables are\", \"are good\", \"good for\", \"for health\", \"fruits are\", \"good source\", \"source of\", \"of vitamins\", \"proteins are\"]\n",
        "  ```\n",
        "  Gove\n",
        "  Then complete the following questions:\n",
        "\n",
        "  1. Term Frequency (`tf`) of **bigrams for message 2**.\n",
        "  2. Inverse Document Frequency (`idf`) of **bigrams for message 2**.\n",
        "  3. `tf-idf` of **bigrams for message 2**.\n",
        "  4. Is the `tf-idf` value for the term \"are good\" equal to 0? If it's 0, can you explain why it makes sense to have 0 for that term?\n",
        "  5. If we consider unigram (1-gram) based bag of words, how many words in these three messages would be assigned a 0 value tf-idf? And please explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1.**\n",
        "TF m2 = [0, 1/5, 0, 0, 1/5, 1/5, 1/5, 1/5, 0]\n",
        "\n",
        "**2.**\n",
        "IDF m2 = [0, 0, 0, 0, ln(3), ln(3), ln(3), ln(3), 0]\n",
        "\n",
        "**3.**\n",
        "TF-IDF m2 = [0, 0, 0, 0, 1/5 ln(3), 1/5 ln(3), 1/5 ln(3), 1/5 ln(3), 0]\n",
        "\n",
        "**4.**\n",
        "Yes, the TF-IDF for \"are good\" is equal to 0. This makes sense, since the bigram \"are good\" appears in every single message, meaning that it is not relevant to consider\n",
        "as it does nothing for differentiating each message. (IDF = 0 since they appear in each document and ln(3/3) = 0)\n",
        "\n",
        "**5.**\n",
        "2 words, as both \"are\" and \"good\" appear in every single document, which will give them both 0 values as their IDF will be equal to 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9IBYCktj40V"
      },
      "source": [
        "\n",
        "2. What does `support` mean in the `classification_report`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`support` in the sklearn `classification_report` means the number of true samples for each class in the dataset. In this case, support for 0 = the number of samples in the dataset for which the true label = 0, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rTOk1aj40V"
      },
      "source": [
        "3. If the training data is highly unbalanced, is `accuracy` a good metric for the classifiers' performance? Does a high accuracy necessarily means a high f1-score? Compare `accuracy` with `f1-score`. Based on your criterion, which type of classifier is better? Please give some justifications to support your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No, `accuracy` is actually a pretty bad metric for an unbalanced dataset. In a highly imbalanced dataset (for example 95% majority class), if a classifier does a really good job predicting the majority class, however fails to correctly predict ANY of the minority class, it will still have a very high accuracy score, however since it does not predict any of the minority class it clearly isn't functioning well as a classifier.\n",
        "\n",
        "A high accuracy score also doies not necessarily imply a high f1-score, as accuracy only takes into account # Correct predictions / Total Samples, ignoring how well a classifier handles false positives and false negatives. F1-score on the other hand balances accuracy and precision, so for my example above, the classifier would have a very high accuracy score, however since it makes a ton of false negatives (assuming minority class = positive in the example), it's F1-score will be quite poor.\n",
        "\n",
        "Lastly, for an unbalanced dataset F1-score is definitely the better metric then accuracy, as it focuses on performances across classes rather than being dominated by the majority class. Based off this criterion, the better classifier is the one with the higher F1-score, even if its accuracy is slightly lower, because it shows stronger performance in detecting all classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Gi_np_j40V"
      },
      "source": [
        "4. For most cases, the amount of bad data is much less than good data. How can you mitigate the data imbalance issue?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing to mitigate the imbalance issues is to use F1-score as your primary metric for model evaluation, since as said above it provides much more meaningful insight into model performance in an unbalanced dataset.\n",
        "\n",
        "Another thing you can do (for models that output probabilities, idk if all libraries support this but the ones I've used do) is to adjust the decision threshhold. Increasing the threshold > 0.5 will increase precision at the cost of recall, and decreasing to <0.5 will do the opposite; increasing recall at the cost of precision. For an unbalanced dataset, lowering the threshold can help allow the model to detect more minority-class samples, but this depends on which are most costly in your exact scenario, false positives or false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5uWmbGhj40V"
      },
      "source": [
        "## Model Improvements (Optional; Not Included in Homework Score)\n",
        "\n",
        "As we have pointed out in the first class, machine learning pipeline does not end with a single iteration of training, testing and evaluation. Here, you will repeat these process with different hyper-parameters and try to improve the spam detection model so that it performs even better.\n",
        "\n",
        "Below are two **potential improvements** that may be beneficial to the performance. You are encouraged to complete either (or both) of them. Your response to these questions are optional and won't be graded, but we will provide feedbacks to your code and analysis if you choose to answer them. You can add code and text cells below the questions as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9evfnknxj40V"
      },
      "source": [
        "1. We use character-level trigrams as tokens during the feature extraction stage, however $N = 3$ is a rather arbitrary choice. What will the performance be like if we use shorter N-grams, such as $N = 1$ and $N = 2$? What about longer N-grams, such as $N = 4$? What if we combine different N-grams, like $N = 1, 2, 3$? Modify the feature extraction code above and play with different (range of) $N$ values. What are **the best (range of) $N$ value on the test set of combined features and SVM classifier**? What metrics did you use to reach this conclusion and why do you think this (range of) $N$ value is the best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjL7aGmj40V"
      },
      "source": [
        "2. During the above training and evaluation process, we are only evaluating three combination of feature extractors and classifiers. Can you try to rewrite the training and evaluation code, such that **every combination** of feature extractors and classifiers can be evaluated? What metrics did you use to reach this conclusion and what is the best combination of feature extractor and classifier in terms of test set performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPMnnVRtj40V"
      },
      "source": [
        "## References\n",
        "\n",
        "1. CRESCI-2017 Dataset: https://botometer.osome.iu.edu/bot-repository/datasets.html\n",
        "2. TF-IDF: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "3. N-grams: https://en.wikipedia.org/wiki/N-gram\n",
        "4. `scikit-learn` documentation: https://scikit-learn.org/stable/modules/classes.html\n",
        "5. Ridge regression: https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
        "6. SVM: https://en.wikipedia.org/wiki/Support-vector_machine\n",
        "7. Lasso: https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
        "8. ElasticNet: https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
        "9. Kernel method: https://en.wikipedia.org/wiki/Kernel_method"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
